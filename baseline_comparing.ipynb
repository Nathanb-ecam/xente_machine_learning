{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "train_data = pd.read_csv('data/training.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "\n",
    "# Separate target from predictors\n",
    "y = train_data.FraudResult\n",
    "X = train_data.drop(['FraudResult'], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                           random_state=0)\n",
    "\n",
    "\n",
    "# train_data.head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at dimensions of our dataset and checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95662 observations\n",
      "15 features\n"
     ]
    }
   ],
   "source": [
    "train_data.size\n",
    "print(f\"{train_data.axes[0].size} observations\")\n",
    "print(f\"{train_data.axes[1].size -1 } features\")\n",
    "# train_data.axes[1].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "print(cols_with_missing)\n",
    "\n",
    "# doesn't affect the dataset since all fields are containing data  \n",
    "# train_data = train_data.dropna(axis=0) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at data types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now that categorical data need to be transposed to numerical data ...For categoricals variables which needs a from of transposition to numeric values. In general, we now that removing data is certainly not a valuable option since we are preventing the learning of our model to use the full training set. Most specific to our xente dataset, we can say that keeping only the numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'TransactionStartTime']\n",
      "['CountryCode', 'Amount', 'Value', 'PricingStrategy']\n"
     ]
    }
   ],
   "source": [
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train[my_cols].copy()\n",
    "X_valid = X_valid[my_cols].copy()\n",
    "\n",
    "print(categorical_cols)\n",
    "print(numerical_cols)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a base pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\"\"\"\n",
    "As said earlier, numerical data is directly interpretable by a model and in our case, we don't even need to deal with imputation since all fields are containing a value \n",
    "Our single preprocessing step will then only concern the categorical variables \n",
    "\"\"\"\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0013066429728740918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to write a conclusion \n",
    "This value should be meaning that our model is very accurate by the model is probably in an overfitting situtaion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking class data proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.79354231729148 % of the transactions are non fraudulent\n"
     ]
    }
   ],
   "source": [
    "full = pd.concat([X_train,y_train],axis=1) \n",
    "not_fraud = full[full.FraudResult==0]\n",
    "fraud = full[full.FraudResult==1]\n",
    "print(f\"{(not_fraud.size/(fraud.size+not_fraud.size))*100} % of the transactions are non fraudulent\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allmost all the transactions registered in our dataset are not fraudulent. We have an imbalanced dataset. Any model with tend to produce overfitting for this type of dataset because it's learning quality will only be based on the non fraudulent transactions, simply ignoring the minority class. Even while testing our model, we are giving him a test data set that also contains a very high majority of non fraudulent cases, this is important to know since the accuracy is directly related to that fact, if we would have for example a test set that would contain only fraudulent transactions, the accuracy value would be very low... (show). That's why better metrics should be used for imbalenced datasets.\n",
    "\n",
    "Since algorithms have all the same general intentions which are to adapt their parameters to reduce their errors to tend to the most accurate models as possible.  We will have to consider different approaches in this specific imbalanced dataset scenario. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better ways to evaluate a model efficiency in the case of a imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score,accuracy_score\n",
    "\n",
    "\n",
    "# compare the model predictions with the real values \n",
    "\n",
    "# calculate the F1 score and recall score\n",
    "def compute_imbalanced_scores(model_preds):\n",
    "    f1 = f1_score(y_valid, model_preds)\n",
    "    recall = recall_score(y_valid, model_preds)\n",
    "    accuracy = accuracy_score(y_valid, model_preds)\n",
    "    # f1 = (2*recall*accuracy)/(recall + accuracy)\n",
    "    return f\" Accuracy score: {accuracy} % \\n F1 score: {f1} % \\n Recall score: {recall} \"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "random_forest = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "models = {\"decision tree\":decision_tree,\"random forest\":random_forest,\"xgb\":xgb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Models(Enum):\n",
    "    DecisionTree = \"decision tree\"\n",
    "    RandomForest = \"random_forest\"\n",
    "    XGB = \"xgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_predictions(trainX,trainY,models = {Models.DecisionTree:decision_tree,Models.RandomForest:random_forest,Models.XGB:xgb} ):\n",
    "    model_predictions = {}\n",
    "    for model_name in models:\n",
    "        print(f\"Trainning {model_name} ...\")\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('model',models[model_name])\n",
    "                                    ])\n",
    "        pipeline.fit(trainX,trainY)\n",
    "        model_preds = pipeline.predict(X_valid)\n",
    "        model_predictions[model_name] = model_preds\n",
    "    return model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning Models.DecisionTree ...\n",
      "Trainning Models.RandomForest ...\n",
      "Trainning Models.XGB ...\n",
      "\n",
      "Original dataset\n",
      "Models.DecisionTree\n",
      " Accuracy score: 0.998641091308211 % \n",
      " F1 score: 0.43478260869565216 % \n",
      " Recall score: 0.2857142857142857 \n",
      "Models.RandomForest\n",
      " Accuracy score: 0.998484294151466 % \n",
      " F1 score: 0.32558139534883723 % \n",
      " Recall score: 0.2 \n",
      "Models.XGB\n",
      " Accuracy score: 0.998588825589296 % \n",
      " F1 score: 0.39999999999999997 % \n",
      " Recall score: 0.2571428571428571 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "models_predictions = compare_models_predictions(X_train,y_train)\n",
    "#models_predictions[\"test\"] = pd.DataFrame(np.zeros(y_train.size))\n",
    "\n",
    "print()\n",
    "print(\"Original dataset\")\n",
    "for model in models_predictions:\n",
    "    print(model)\n",
    "    scores = compute_imbalanced_scores(models_predictions[model])\n",
    "    print(scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Several options : oversampling vs downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xnew = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "not_fraud = Xnew[Xnew.FraudResult==0]\n",
    "fraud = Xnew[Xnew.FraudResult==1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersample majority class : train the model with a smaller amount of fraudulent transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FraudResult\n",
       "0    158\n",
       "1    158\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled.FraudResult.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning Models.DecisionTree ...\n",
      "Trainning Models.RandomForest ...\n",
      "Trainning Models.XGB ...\n",
      "\n",
      "Downsampled the non fraudulent transactions\n",
      "Models.DecisionTree\n",
      " Accuracy score: 0.9491977212146553 % \n",
      " F1 score: 0.06538461538461537 % \n",
      " Recall score: 0.9714285714285714 \n",
      "Models.RandomForest\n",
      " Accuracy score: 0.8715308629070193 % \n",
      " F1 score: 0.027689873417721517 % \n",
      " Recall score: 1.0 \n",
      "Models.XGB\n",
      " Accuracy score: 0.8497883238383944 % \n",
      " F1 score: 0.023777173913043476 % \n",
      " Recall score: 1.0 \n"
     ]
    }
   ],
   "source": [
    "# creating an undersampled pipeline \n",
    "down_y_train = downsampled.FraudResult\n",
    "down_X_train = downsampled.drop('FraudResult', axis=1)\n",
    "\n",
    "downsampled_predictions = compare_models_predictions(down_X_train,down_y_train)\n",
    "print()\n",
    "print(\"Downsampled the non fraudulent transactions\")\n",
    "for model in downsampled_predictions:\n",
    "    print(model)\n",
    "    scores = compute_imbalanced_scores(downsampled_predictions[model])\n",
    "    print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FraudResult\n",
       "0    76371\n",
       "1    76371\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "upsampled.FraudResult.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_y_train = upsampled.FraudResult\n",
    "up_X_train = upsampled.drop('FraudResult', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning Models.DecisionTree ...\n",
      "Trainning Models.RandomForest ...\n",
      "Trainning Models.XGB ...\n",
      "\n",
      "Upsampled the fraudulent transactions\n",
      "Models.DecisionTree\n",
      " Accuracy score: 0.9926827993519051 % \n",
      " F1 score: 0.29292929292929293 % \n",
      " Recall score: 0.8285714285714286 \n",
      "Models.RandomForest\n",
      " Accuracy score: 0.998536559870381 % \n",
      " F1 score: 0.44 % \n",
      " Recall score: 0.3142857142857143 \n",
      "Models.XGB\n",
      " Accuracy score: 0.9889719333089426 % \n",
      " F1 score: 0.19771863117870722 % \n",
      " Recall score: 0.7428571428571429 \n"
     ]
    }
   ],
   "source": [
    "# creating a pipeline for the oversampling of farudulent transactions\n",
    "oversampled_predictions = compare_models_predictions(up_X_train,up_y_train)\n",
    "print()\n",
    "print(\"Upsampled the fraudulent transactions\")\n",
    "for model in oversampled_predictions:\n",
    "    print(model)\n",
    "    scores = compute_imbalanced_scores(oversampled_predictions[model])\n",
    "    print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models.XGB\n",
      "\n",
      "original dataset\n",
      " Accuracy score: 0.998588825589296 % \n",
      " F1 score: 0.39999999999999997 % \n",
      " Recall score: 0.2571428571428571 \n",
      "\n",
      "upsampled\n",
      " Accuracy score: 0.9889719333089426 % \n",
      " F1 score: 0.19771863117870722 % \n",
      " Recall score: 0.7428571428571429 \n",
      "\n",
      "downsampled\n",
      " Accuracy score: 0.8497883238383944 % \n",
      " F1 score: 0.023777173913043476 % \n",
      " Recall score: 1.0 \n"
     ]
    }
   ],
   "source": [
    "model_name = Models.XGB\n",
    "scores = compute_imbalanced_scores(models_predictions[model_name])\n",
    "print(f\" {model_name}\")\n",
    "print()\n",
    "print(\"original dataset\")\n",
    "print(scores)\n",
    "print()\n",
    "print(\"upsampled\")\n",
    "scores = compute_imbalanced_scores(oversampled_predictions[model_name])\n",
    "print(scores)\n",
    "print()\n",
    "print(\"downsampled\")\n",
    "scores = compute_imbalanced_scores(downsampled_predictions[model_name])\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the obtained results for our 3 bases models, downsampling seems to be the best approach for fitting our dataset. I will then train a model on the downsampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "opt_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "\n",
    "opt_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('model',opt_model)\n",
    "                            ])\n",
    "opt_pipeline.fit(down_X_train,down_y_train)\n",
    "opt_model_preds = opt_pipeline.predict(X_valid)\n",
    "\n",
    "compute_imbalanced_scores(opt_model_preds)\n",
    "\n",
    "# submission \n",
    "#opt_preds = pd.DataFrame(opt_pipeline.predict(test_data),columns=['FraudResult']).to_csv('prediction.csv')\n",
    "\n",
    "opt_preds = opt_pipeline.predict(test_data)\n",
    "print(opt_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using best f1 score from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Accuracy score: 0.998641091308211 % \\n F1 score: 0.43478260869565216 % \\n Recall score: 0.2857142857142857 '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(models.keys())\n",
    "tree_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('model',models[\"decision tree\"])\n",
    "                            ])\n",
    "tree_pipeline.fit(X_train,y_train)\n",
    "tree_preds = tree_pipeline.predict(X_valid)\n",
    "\n",
    "compute_imbalanced_scores(tree_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
